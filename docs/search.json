[
  {
    "objectID": "calculators-for-words.html#this-paper-is-available-as-a-preprint",
    "href": "calculators-for-words.html#this-paper-is-available-as-a-preprint",
    "title": "Is the ‘Calculator for Words’ analogy useful for communicating about LLMs?",
    "section": "This paper is available as a preprint",
    "text": "This paper is available as a preprint\nFeedback would be delightful.\n\n\n\n\n\ndoi.org/10.5281/zenodo.12602858\n\n\n\n\nThis presentation is CC-BY.\nCode available at https://github.com/Denubis/calculator-for-words-presentation\nPresentation available at: https://denubis.github.io/calculator-for-words-presentation/"
  },
  {
    "objectID": "calculators-for-words.html#understanding-the-capabilities-of-technology-is-not-a-new-problem",
    "href": "calculators-for-words.html#understanding-the-capabilities-of-technology-is-not-a-new-problem",
    "title": "Is the ‘Calculator for Words’ analogy useful for communicating about LLMs?",
    "section": "Understanding the capabilities of technology is not a new problem",
    "text": "Understanding the capabilities of technology is not a new problem\n\nPray, Mr. Babbage, if you put into the machine wrong figures, will the right answers come out? (Babbage 1864)\n\n\nFor an LLM: the answer is YES.\n\nLarge language models (LLMs) are fundamentally different from search engines, functioning more as ‘vibe-machines’ than information retrieval systems. (Ballsun-Stanton and Hipólito 2024)"
  },
  {
    "objectID": "calculators-for-words.html#motivation-and-aim",
    "href": "calculators-for-words.html#motivation-and-aim",
    "title": "Is the ‘Calculator for Words’ analogy useful for communicating about LLMs?",
    "section": "Motivation and Aim",
    "text": "Motivation and Aim\n\nWhy are we here: The need for effective LLM comprehension.\nIs the ‘calculator for words’ analogy the most effective one?\nHow do we teach, communicate, and write policy about these tools without effective analogies to communicate their major capabilities and limitations?"
  },
  {
    "objectID": "calculators-for-words.html#our-goal",
    "href": "calculators-for-words.html#our-goal",
    "title": "Is the ‘Calculator for Words’ analogy useful for communicating about LLMs?",
    "section": "Our goal",
    "text": "Our goal\nFind an effective analogy to communicate LLM affordances to avoid error-prone usage patterns."
  },
  {
    "objectID": "calculators-for-words.html#the-problem",
    "href": "calculators-for-words.html#the-problem",
    "title": "Is the ‘Calculator for Words’ analogy useful for communicating about LLMs?",
    "section": "The Problem",
    "text": "The Problem\n\nToo many conflicting stories about “AI”\nAnthropomorphization: They respond using “I”\nSkeuomorphic lenses vs. understanding true limitations and capabilities"
  },
  {
    "objectID": "calculators-for-words.html#willison-on-llms",
    "href": "calculators-for-words.html#willison-on-llms",
    "title": "Is the ‘Calculator for Words’ analogy useful for communicating about LLMs?",
    "section": "Willison on LLMs",
    "text": "Willison on LLMs\n\nOne of the most pervasive mistakes I see people using with large language model tools like ChatGPT is trying to use them as a search engine. … I like to think of language models like ChatGPT as a calculator for words. … Want them to work with specific facts? Paste those into the language model as part of your original prompt! (Willison 2023)"
  },
  {
    "objectID": "calculators-for-words.html#unreliability-and-locus-of-control",
    "href": "calculators-for-words.html#unreliability-and-locus-of-control",
    "title": "Is the ‘Calculator for Words’ analogy useful for communicating about LLMs?",
    "section": "Unreliability and Locus of Control",
    "text": "Unreliability and Locus of Control\nThe analogy ‘calculator for words’:\n\nMoves locus of control to the user’s perspective\nUser input is primary, not the ‘creative’ output of the machine\nGrounding inputs to reduce confabulation\nMaps to prior affordances users expect"
  },
  {
    "objectID": "calculators-for-words.html#difficulty-of-effective-use",
    "href": "calculators-for-words.html#difficulty-of-effective-use",
    "title": "Is the ‘Calculator for Words’ analogy useful for communicating about LLMs?",
    "section": "Difficulty of Effective Use",
    "text": "Difficulty of Effective Use\n\nUsing LLMs effectively is deceptively difficult\nRequires building an accurate mental model of capabilities and limitations\nUsers need to spend time with LLMs to understand their potential and pitfalls"
  },
  {
    "objectID": "calculators-for-words.html#llms-vs.-search-engines",
    "href": "calculators-for-words.html#llms-vs.-search-engines",
    "title": "Is the ‘Calculator for Words’ analogy useful for communicating about LLMs?",
    "section": "LLMs vs. Search Engines",
    "text": "LLMs vs. Search Engines\nArgument 2.1 (Modus ponens) (Ballsun-Stanton and Hipólito 2024):\nP1. If an AI system lacks inherent intentional agency in its core operation (token inference) and its outputs are primarily bounded by human-defined service layers, then it is fundamentally different from search engines that link to intentionally created content.\n\nP2. LLMs lack inherent intentional agency in their core operation (token inference), and their outputs are primarily determined by human-defined service layers (system prompts and post-hoc interactions applied to token inference).\n\n\nC. Therefore, LLMs are fundamentally different from search engines that link to intentionally created content."
  },
  {
    "objectID": "calculators-for-words.html#buccis-response",
    "href": "calculators-for-words.html#buccis-response",
    "title": "Is the ‘Calculator for Words’ analogy useful for communicating about LLMs?",
    "section": "Bucci’s Response",
    "text": "Bucci’s Response\n\nTo put it differently, a calculator has a well-defined, well-scoped set of use cases, a well-defined, well-scoped user interface, and a set of well-understood and expected behaviors that occur in response to manipulations of that interface. … Large language models, when used to drive chatbots or similar interactive text-generation systems, have none of those qualities. They have an open-ended set of unspecified use cases. (Bucci 2023)"
  },
  {
    "objectID": "calculators-for-words.html#determinism-and-affordances",
    "href": "calculators-for-words.html#determinism-and-affordances",
    "title": "Is the ‘Calculator for Words’ analogy useful for communicating about LLMs?",
    "section": "Determinism and Affordances",
    "text": "Determinism and Affordances\nTheorem 3.1 (Ballsun-Stanton and Hipólito 2024):\nA calculator, upon receiving invalid input, will give the user an error. A LLM, upon receiving invalid input, will give the user a valid response.\n\nLLMs lack the deterministic nature of calculators\nThe interface of LLMs is deceptively simple, hiding complex and often unpredictable behavior\nAffordances of LLMs are vastly different from those of calculators"
  },
  {
    "objectID": "calculators-for-words.html#chatgpt-is-bullshit-and-appropriateness-of-use",
    "href": "calculators-for-words.html#chatgpt-is-bullshit-and-appropriateness-of-use",
    "title": "Is the ‘Calculator for Words’ analogy useful for communicating about LLMs?",
    "section": "“ChatGPT is Bullshit” and Appropriateness of Use",
    "text": "“ChatGPT is Bullshit” and Appropriateness of Use\n\nLLMs generate text without adherence to an underlying reality (Hicks, Humphries, and Slater 2024)\nOutput is a stream of tokens with the highest statistical likelihood, appearing as coherent thought\nUnlike calculators, which have clear appropriate use cases, LLMs’ appropriate applications are still being defined\nTherefore: The analogy fails to capture the open-ended nature of LLM interactions and outputs"
  },
  {
    "objectID": "calculators-for-words.html#the-pragmatics-of-experience",
    "href": "calculators-for-words.html#the-pragmatics-of-experience",
    "title": "Is the ‘Calculator for Words’ analogy useful for communicating about LLMs?",
    "section": "The Pragmatics of Experience",
    "text": "The Pragmatics of Experience\n\nWhen I speak in front of groups and ask them to raise their hands if they used the free version of ChatGPT, almost every hand goes up. When I ask the same group how many use GPT-4, almost no one raises their hand. I increasingly think the decision of OpenAI to make the “bad” AI free is causing people to miss why AI seems like such a huge deal to a minority of people that use advanced systems and elicits a shrug from everyone else. (Mollick 2023)\n\n\nThe gap in user experience between different LLM versions affects perception and understanding\nThis disparity influences how we communicate about and conceptualize LLMs"
  },
  {
    "objectID": "calculators-for-words.html#evaluating-the-calculator-for-words-analogy",
    "href": "calculators-for-words.html#evaluating-the-calculator-for-words-analogy",
    "title": "Is the ‘Calculator for Words’ analogy useful for communicating about LLMs?",
    "section": "Evaluating the ‘Calculator for Words’ Analogy",
    "text": "Evaluating the ‘Calculator for Words’ Analogy\nIntuitions:\n\nThe analogy falls apart on deeper inspection\nLacks useful ontological or epistemological similarities with LLMs"
  },
  {
    "objectID": "calculators-for-words.html#evaluating-the-calculator-for-words-analogy-1",
    "href": "calculators-for-words.html#evaluating-the-calculator-for-words-analogy-1",
    "title": "Is the ‘Calculator for Words’ analogy useful for communicating about LLMs?",
    "section": "Evaluating the ‘Calculator for Words’ Analogy",
    "text": "Evaluating the ‘Calculator for Words’ Analogy\nPedagogical utility:\n\nUseful for teaching attention to context window and token inference\nHelps adjust audiences’ epistemic orientation to LLM interfaces"
  },
  {
    "objectID": "calculators-for-words.html#evaluating-the-calculator-for-words-analogy-2",
    "href": "calculators-for-words.html#evaluating-the-calculator-for-words-analogy-2",
    "title": "Is the ‘Calculator for Words’ analogy useful for communicating about LLMs?",
    "section": "Evaluating the ‘Calculator for Words’ Analogy",
    "text": "Evaluating the ‘Calculator for Words’ Analogy\nComprehensibility:\n\nRequires specific interpretation to be useful\nMay lead to misunderstandings if taken too literally"
  },
  {
    "objectID": "calculators-for-words.html#maps-of-no-territory-a-new-analogy",
    "href": "calculators-for-words.html#maps-of-no-territory-a-new-analogy",
    "title": "Is the ‘Calculator for Words’ analogy useful for communicating about LLMs?",
    "section": "Maps of No Territory: A New Analogy",
    "text": "Maps of No Territory: A New Analogy\n\nInspired by Borges’ “On Exactitude in Science” (Borges and Hurley 1998)\nLLMs as maps without a corresponding territory\nAims to provide more nuanced intuitions for general audiences\n\n\nLLMs, as our proposed map-analogues, generate language based on statistical relationships learned from vast amounts of text, creating abstract representations of language patterns and structures. (Ballsun-Stanton and Hipólito 2024)"
  },
  {
    "objectID": "calculators-for-words.html#traces-on-maps",
    "href": "calculators-for-words.html#traces-on-maps",
    "title": "Is the ‘Calculator for Words’ analogy useful for communicating about LLMs?",
    "section": "Traces on maps",
    "text": "Traces on maps\n\n“Training” an LLM:\n\nit stores the representations of the statistical relationships between tokens.\nIn our map-metaphor, we can call them traces.\n\nThe relationships stored in the model’s weights are a map of no territory.\n\nA trace is a correspondence of a sign/token output by an LLM which has a referent useful to the user."
  },
  {
    "objectID": "calculators-for-words.html#llms-as-maps-of-no-territory",
    "href": "calculators-for-words.html#llms-as-maps-of-no-territory",
    "title": "Is the ‘Calculator for Words’ analogy useful for communicating about LLMs?",
    "section": "LLMs as Maps of No Territory",
    "text": "LLMs as Maps of No Territory\nArgument 5.1 (Modus ponens) (Ballsun-Stanton and Hipólito 2024):\nP1. If LLMs generate language based on abstract representations without direct access to real-world referents, then LLMs are like maps of no territory.\n\nP2. LLMs generate language based on abstract representations without direct access to real-world referents.\n\n\nC. Therefore, LLMs are like maps of no territory."
  },
  {
    "objectID": "calculators-for-words.html#effective-interaction-and-navigation",
    "href": "calculators-for-words.html#effective-interaction-and-navigation",
    "title": "Is the ‘Calculator for Words’ analogy useful for communicating about LLMs?",
    "section": "Effective Interaction and Navigation",
    "text": "Effective Interaction and Navigation\n\nInteracting with LLMs is like being a librarian in Borges’ Infinite Library\nRequires skillful navigation to find meaningful content\nThree levels of (mental) mapping:\n\nExpertise Map: User’s foundational understanding\nIncomplete but Sufficient Map: Framework for effective engagement\nMap of a Map of no Territory: Abstract representations within the LLM"
  },
  {
    "objectID": "calculators-for-words.html#conclusion",
    "href": "calculators-for-words.html#conclusion",
    "title": "Is the ‘Calculator for Words’ analogy useful for communicating about LLMs?",
    "section": "Conclusion",
    "text": "Conclusion\n\nThe ‘calculator for words’ analogy serves as an effective negative heuristic\n\nDiscourages treating LLMs as search engines or fact repositories\nFalls short in providing positive intuition for effective LLM utilization\n\n‘Maps of no territory’ offers a more comprehensive understanding\n\nCaptures the nature, capabilities, and limitations of LLMs\nEncourages more informed and responsible engagement\n\nEffective use of LLMs requires:\n\nSkillful interpretation of traces provided by LLM interaction\nDeveloping a framework for effective engagement\nUnderstanding that outputs reflect training data, not direct representations of reality"
  },
  {
    "objectID": "calculators-for-words.html#references",
    "href": "calculators-for-words.html#references",
    "title": "Is the ‘Calculator for Words’ analogy useful for communicating about LLMs?",
    "section": "References",
    "text": "References\n\n\n\n\nhttps://doi.org/10.5281/zenodo.12602858 • © 2024, License: CC-BY • https://denubis.github.io/calculator-for-words-presentation/\n\n\n\n\nBabbage, Charles. 1864. “Passages from the Life of a Philosopher.” https://www.gutenberg.org/files/57532/57532-h/57532-h.htm.\n\n\nBallsun-Stanton, Brian, and Inês Hipólito. 2024. “Is the ’Calculator for Words’ analogy useful for communicating about LLMs?” Zenodo. https://doi.org/10.5281/zenodo.12602858.\n\n\nBorges, Jorge Luis, and Andrew Hurley. 1998. Collected Fictions. Penguin Classics Deluxe Edition. New York, NY: Penguin Books.\n\n\nBucci, Anthony. 2023. “Word Calculators Don’t Add Up.” https://bucci.onl/notes/Word-calculators-dont-add-up.\n\n\nHicks, Michael Townsen, James Humphries, and Joe Slater. 2024. “ChatGPT Is Bullshit.” Ethics and Information Technology 26 (2): 38. https://doi.org/10.1007/s10676-024-09775-5.\n\n\nMollick, Ethan. 2023. “An Opinionated Guide to Which AI to Use: ChatGPT Anniversary Edition.” https://www.oneusefulthing.org/p/an-opinionated-guide-to-which-ai.\n\n\nWillison, Simon. 2023. “Think of Language Models Like ChatGPT as a ‘Calculator for Words’.” Simon Willison’s Weblog. https://simonwillison.net/2023/Apr/2/calculator-for-words/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Calculator for Words Presentation, IACAP 2024 Eugene, OR",
    "section": "",
    "text": "Preprint\nPresentation\nCode"
  }
]